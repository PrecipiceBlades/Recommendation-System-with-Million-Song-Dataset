{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "sqlContext = SQLContext(sc)\n",
    "dfMSD = sqlContext.read.parquet(\"s3a://msdbucket/parquet\")\n",
    "dfMSD.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Load taste profile dataset and convert to df with defined schema\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "rddTaste = sc.textFile(\"file:///home/hadoop/train_triplets.txt\").map(lambda x: x.split(\"\\t\")).map(lambda p: (p[0], p[1], int(p[2])))\n",
    "schemaTaste = StructType([StructField(\"user\", StringType(), True), StructField(\"song\", StringType(), True), StructField(\"playCount\", IntegerType(), True)])\n",
    "dfTaste = sqlContext.createDataFrame(rddTaste, schema=schemaTaste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql import functions as F\n",
    "# change ids from strings to integers\n",
    "user_change = dfTaste.select('user').distinct().select('user', F.monotonically_increasing_id().alias('new_user'))\n",
    "song_change = dfTaste.select('song').distinct().select('song', F.monotonically_increasing_id().alias('new_song'))\n",
    "\n",
    "# get total unique users and songs\n",
    "unique_users = user_change.count()\n",
    "unique_songs = song_change.count()\n",
    "print('Number of unique users: {0}'.format(unique_users))\n",
    "print('Number of unique songs: {0}'.format(unique_songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "# Run string indexer on serveral column\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(dfTaste) for column in [\"user\", \"song\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "dfTaste_idx = pipeline.fit(dfTaste).transform(dfTaste)\n",
    "\n",
    "dfTaste_idx.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Cast index column to Integer\n",
    "dfTaste_idx = dfTaste_idx.withColumn(\"user_index\", dfTaste_idx['user_index'].cast(IntegerType()))\n",
    "dfTaste_idx = dfTaste_idx.withColumn(\"song_index\", dfTaste_idx['song_index'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# cache\n",
    "tasteDf_with_songId = dfTaste_idx\n",
    "tasteDf_with_songId.cache()\n",
    "tasteDf_with_songId.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# We'll hold out 60% for training, 20% of our data for validation, and leave 20% for testing\n",
    "seed = 1800009193L\n",
    "(split_60_df, split_a_20_df, split_b_20_df) = tasteDf_with_songId.randomSplit([0.6, 0.2, 0.2], seed = seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "training_df = split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print('Training: {0}, validation: {1}, test: {2}\\n'.format(\n",
    "  training_df.count(), validation_df.count(), test_df.count())\n",
    ")\n",
    "training_df.show(3)\n",
    "validation_df.show(3)\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#Number of plays needs to be double type, not integers\n",
    "validation_df = validation_df.withColumn(\"playCount\", validation_df[\"playCount\"].cast(DoubleType()))\n",
    "validation_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Alternating least squares\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Let's initialize our ALS learner\n",
    "als = ALS()\n",
    "\n",
    "# Now set the parameters for the method\n",
    "als.setMaxIter(5)\\\n",
    "   .setSeed(seed)\\\n",
    "   .setItemCol(\"song_index\")\\\n",
    "   .setRatingCol(\"playCount\")\\\n",
    "   .setUserCol(\"user_index\")\n",
    "\n",
    "# Create regression evaluator\n",
    "reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"playCount\", metricName=\"rmse\")\n",
    "\n",
    "# Hyperparameter tuning to find best rank and reg param\n",
    "tolerance = 0.03\n",
    "ranks = [4, 8, 12, 16]\n",
    "regParams = [0.15, 0.2, 0.25]\n",
    "errors = [[0]*len(ranks)]*len(regParams)\n",
    "models = [[0]*len(ranks)]*len(regParams)\n",
    "err = 0\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "i = 0\n",
    "for regParam in regParams:\n",
    "  j = 0\n",
    "  for rank in ranks:\n",
    "    # Set the rank here:\n",
    "    als.setParams(rank = rank, regParam = regParam)\n",
    "    # Create the model with these parameters.\n",
    "    model = als.fit(training_df)\n",
    "    # Run the model to create a prediction. Predict against the validation_df.\n",
    "    predict_df = model.transform(validation_df)\n",
    "\n",
    "    # Remove NaN values from prediction (due to SPARK-14489)\n",
    "    predicted_plays_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "    predicted_plays_df = predicted_plays_df.withColumn(\"prediction\", F.abs(F.round(predicted_plays_df[\"prediction\"],0)))\n",
    "    # Run the previously created RMSE evaluator, reg_eval, on the predicted_ratings_df DataFrame\n",
    "    error = reg_eval.evaluate(predicted_plays_df)\n",
    "    errors[i][j] = error\n",
    "    models[i][j] = model\n",
    "    print 'For rank %s, regularization parameter %s the RMSE is %s' % (rank, regParam, error)\n",
    "    if error < min_error:\n",
    "      min_error = error\n",
    "      best_params = [i,j]\n",
    "    j += 1\n",
    "  i += 1\n",
    "\n",
    "als.setRegParam(regParams[best_params[0]])\n",
    "als.setRank(ranks[best_params[1]])\n",
    "print 'The best model was trained with regularization parameter %s' % regParams[best_params[0]]\n",
    "print 'The best model was trained with rank %s' % ranks[best_params[1]]\n",
    "my_model = models[best_params[0]][best_params[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "predicted_plays_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Testing the model\n",
    "# In ML Pipelines, this next step has a bug that produces unwanted NaN values. We\n",
    "# have to filter them out. See https://issues.apache.org/jira/browse/SPARK-14489\n",
    "\n",
    "test_df = test_df.withColumn(\"playCount\", test_df[\"playCount\"].cast(DoubleType()))\n",
    "predict_df = my_model.transform(test_df)\n",
    "\n",
    "# Remove NaN values from prediction (due to SPARK-14489)\n",
    "predicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "\n",
    "# Round floats to whole numbers\n",
    "predicted_test_df = predicted_test_df.withColumn(\"prediction\", F.abs(F.round(predicted_test_df[\"prediction\"],0)))\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_df DataFrame\n",
    "test_RMSE = reg_eval.evaluate(predicted_test_df)\n",
    "\n",
    "print('The model had a RMSE on the test set of {0}'.format(test_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "# Comparing the model with the error from a test set where every rating is the averge number of plays from the training set\n",
    "avg_plays_df = training_df.groupBy().avg('Plays').select(F.round('avg(Plays)'))\n",
    "\n",
    "avg_plays_df.show(3)\n",
    "# Extract the average rating value. (This is row 0, column 0.)\n",
    "training_avg_plays = avg_plays_df.collect()[0][0]\n",
    "\n",
    "print('The average number of plays in the dataset is {0}'.format(training_avg_plays))\n",
    "\n",
    "# Add a column with the average rating\n",
    "test_for_avg_df = test_df.withColumn('prediction', F.lit(training_avg_plays))\n",
    "\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the test_for_avg_df DataFrame\n",
    "test_avg_RMSE = reg_eval.evaluate(test_for_avg_df)\n",
    "\n",
    "print(\"The RMSE on the average set is {0}\".format(test_avg_RMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
